from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import requests
import os
import logging
from typing import List, Dict

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
app = FastAPI(
    title="ü§ñ AI Assistant Pro",
    description="–£–º–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫ –Ω–∞ Hugging Face",
    version="1.0.0"
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
HF_TOKEN = os.getenv("HF_TOKEN")
HF_API_URL = "https://api-inference.huggingface.co/models/microsoft/DialoGPT-medium"

# –ú–æ–¥–µ–ª–∏ –¥–∞–Ω–Ω—ã—Ö
class ChatRequest(BaseModel):
    message: str
    history: List[Dict] = []

class ChatResponse(BaseModel):
    response: str
    history: List[Dict]

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç AI
async def get_ai_response(message: str, history: List[Dict]) -> str:
    """–ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç –æ—Ç Hugging Face API"""
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ–∫–µ–Ω–∞
    if not HF_TOKEN:
        logger.error("HF_TOKEN not configured")
        return "üîß AI —Å–∏—Å—Ç–µ–º–∞ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç—Å—è. –î–æ–±–∞–≤—å—Ç–µ HF_TOKEN –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ Vercel"
    
    try:
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏
        prompt = ""
        for msg in history[-4:]:  # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 4 —Å–æ–æ–±—â–µ–Ω–∏—è
            role = "User" if msg["role"] == "user" else "Assistant"
            prompt += f"{role}: {msg['content']}\n"
        
        prompt += f"User: {message}\nAssistant:"
        
        logger.info(f"Sending request to HF API: {prompt[:100]}...")
        
        # –ó–∞–≥–æ–ª–æ–≤–∫–∏ –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        headers = {
            "Authorization": f"Bearer {HF_TOKEN}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "inputs": prompt,
            "parameters": {
                "max_new_tokens": 150,
                "temperature": 0.7,
                "do_sample": True,
                "return_full_text": False
            }
        }
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å
        response = requests.post(
            HF_API_URL, 
            headers=headers, 
            json=payload, 
            timeout=30
        )
        
        logger.info(f"HF API response status: {response.status_code}")
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Ç–≤–µ—Ç
        if response.status_code == 200:
            result = response.json()
            if isinstance(result, list) and len(result) > 0:
                ai_text = result[0].get('generated_text', '').strip()
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
                if "Assistant:" in ai_text:
                    return ai_text.split("Assistant:")[-1].strip()
                return ai_text
            else:
                return "ü§ñ –ù–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –æ—Ç–≤–µ—Ç AI"
                
        elif response.status_code == 503:
            logger.warning("Model is loading")
            return "ü§ñ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è... –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —á–µ—Ä–µ–∑ 30 —Å–µ–∫—É–Ω–¥!"
            
        else:
            logger.error(f"HF API error: {response.status_code} - {response.text}")
            return f"‚ùå –û—à–∏–±–∫–∞ AI —Å–µ—Ä–≤–∏—Å–∞: {response.status_code}"
            
    except requests.exceptions.Timeout:
        logger.error("HF API timeout")
        return "‚è∞ –¢–∞–π–º–∞—É—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ AI —Å–µ—Ä–≤–∏—Å—É"
        
    except requests.exceptions.ConnectionError:
        logger.error("HF API connection error")
        return "üîå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ AI —Å–µ—Ä–≤–∏—Å—É"
        
    except Exception as e:
        logger.error(f"Unexpected error: {str(e)}")
        return f"‚ö†Ô∏è –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {str(e)}"

# –≠–Ω–¥–ø–æ–∏–Ω—Ç—ã API
@app.post("/chat", response_model=ChatResponse)
async def chat_endpoint(request: ChatRequest):
    """–û—Å–Ω–æ–≤–Ω–æ–π —ç–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è —á–∞—Ç–∞"""
    try:
        logger.info(f"Chat request: {request.message}")
        
        # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç –æ—Ç AI
        ai_response = await get_ai_response(request.message, request.history)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –∏—Å—Ç–æ—Ä–∏—é
        new_history = request.history + [
            {"role": "user", "content": request.message},
            {"role": "assistant", "content": ai_response}
        ]
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 8 —Å–æ–æ–±—â–µ–Ω–∏–π
        new_history = new_history[-8:]
        
        logger.info(f"Chat response: {ai_response[:100]}...")
        
        return ChatResponse(
            response=ai_response,
            history=new_history
        )
        
    except Exception as e:
        logger.error(f"Chat endpoint error: {str(e)}")
        return ChatResponse(
            response="–ü—Ä–∏–≤–µ—Ç! –Ø –≤–∞—à AI –ø–æ–º–æ—â–Ω–∏–∫. –ü—Ä–æ–∏–∑–æ—à–ª–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞.",
            history=request.history
        )

@app.get("/")
async def root():
    """–ö–æ—Ä–Ω–µ–≤–æ–π —ç–Ω–¥–ø–æ–∏–Ω—Ç"""
    return {
        "status": "active", 
        "service": "AI Assistant Pro",
        "ai_provider": "Hugging Face",
        "model": "DialoGPT-medium"
    }

@app.get("/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞"""
    return {
        "status": "healthy",
        "timestamp": "2024-10-27",
        "hf_token_configured": bool(os.getenv("HF_TOKEN"))
    }

@app.get("/test")
async def test_ai():
    """–¢–µ—Å—Ç–æ–≤—ã–π —ç–Ω–¥–ø–æ–∏–Ω—Ç"""
    try:
        test_response = await get_ai_response("–ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ –¥–µ–ª–∞?", [])
        return {
            "test": "success",
            "ai_response": test_response,
            "hf_token_configured": bool(os.getenv("HF_TOKEN"))
        }
    except Exception as e:
        return {
            "test": "error",
            "error": str(e),
            "hf_token_configured": bool(os.getenv("HF_TOKEN"))
        }

# –î–ª—è Vercel —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è app
app = app

# –î–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
